Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading pytorch/1.0.0
  Loading requirement: cuda/10.0 cudnn/7.4
rm: cannot remove ‘*.best_trans’: No such file or directory
rm: cannot remove ‘*.beam_trans’: No such file or directory
Traceback (most recent call last):
  File "/afs/crc.nd.edu/x86_64_linux/p/pytorch/1.0.0/build-new/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/afs/crc.nd.edu/x86_64_linux/p/pytorch/1.0.0/build-new/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/afs/crc.nd.edu/group/nlp/04/cmcdona8/witwicky2/nmt/__main__.py", line 30, in <module>
    trainer.train()
  File "/afs/crc.nd.edu/group/nlp/04/cmcdona8/witwicky2/nmt/train.py", line 211, in train
    self.run_log(b, e, batch_data)
  File "/afs/crc.nd.edu/group/nlp/04/cmcdona8/witwicky2/nmt/train.py", line 117, in run_log
    ret = self.model(src_toks_cuda, src_trees, trg_toks_cuda, targets_cuda) # calls some functions, ends up calling model.forward()
  File "/afs/crc.nd.edu/x86_64_linux/p/pytorch/1.0.0/build-new/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/group/nlp/04/cmcdona8/witwicky2/nmt/model.py", line 121, in forward
    encoder_inputs = self.get_input(src_toks, src_trees)
  File "/afs/crc.nd.edu/group/nlp/04/cmcdona8/witwicky2/nmt/model.py", line 111, in get_input
    pos_embeds = torch.stack([tree.get_pos_embedding(self.pos_embedding_mu_l, self.pos_embedding_mu_r, self.pos_embedding_lambda, toks.size()[-1], self.config['embed_dim']) for tree in trees]) # [bsz, max_len, embed_dim]
  File "/afs/crc.nd.edu/group/nlp/04/cmcdona8/witwicky2/nmt/model.py", line 111, in <listcomp>
    pos_embeds = torch.stack([tree.get_pos_embedding(self.pos_embedding_mu_l, self.pos_embedding_mu_r, self.pos_embedding_lambda, toks.size()[-1], self.config['embed_dim']) for tree in trees]) # [bsz, max_len, embed_dim]
  File "/afs/crc.nd.edu/group/nlp/04/cmcdona8/witwicky2/nmt/tree.py", line 102, in get_pos_embedding
    assert False, "Inside norm: {}\nOutside norm: {}".format(inside.map(lambda i: i.norm()), outside.map(lambda o: o.norm()))
AssertionError: Inside norm: tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) ( tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) ( tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) ) ( tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) ( tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) ) ) ( tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) ) tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>) ) tensor(1.0103, device='cuda:0', grad_fn=<NormBackward0>)
Outside norm: tensor(1.3215, device='cuda:0', grad_fn=<NormBackward0>) ( tensor(1.1809, device='cuda:0', grad_fn=<NormBackward0>) ( tensor(1.1369, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.3054, device='cuda:0', grad_fn=<NormBackward0>) ) ( tensor(1.0140, device='cuda:0', grad_fn=<NormBackward0>) ( tensor(1.0408, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.0689, device='cuda:0', grad_fn=<NormBackward0>) ) ) ( tensor(1.0345, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.0960, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.1014, device='cuda:0', grad_fn=<NormBackward0>) ) tensor(1.2388, device='cuda:0', grad_fn=<NormBackward0>) tensor(1.2738, device='cuda:0', grad_fn=<NormBackward0>) ) tensor(1.2980, device='cuda:0', grad_fn=<NormBackward0>)
